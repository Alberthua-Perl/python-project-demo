{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4470cfc-8f69-4531-89ac-16de97bbeb32",
   "metadata": {},
   "source": [
    "# **深度学习系列：梯度 & 散度**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a5622-55c6-42bf-b5a2-0e1faff25706",
   "metadata": {},
   "source": [
    "## **1. 梯度**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd8bf0-6bb0-4334-9b4b-5e20e607ca23",
   "metadata": {},
   "source": [
    "以下是关于 **多元函数的梯度** 和 **梯度下降** 的详细定义和解释，结合数学公式与几何意义说明：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 梯度（Gradient）的定义**\n",
    "对于多元函数 $f(\\mathbf{x})$，其中 $\\mathbf{x} = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$，其梯度是一个向量，由函数对所有自变量的偏导数组成：\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)^\\top\n",
    "$$\n",
    "\n",
    "#### **几何意义**：\n",
    "- 梯度方向是函数在该点 **增长最快** 的方向。\n",
    "- 梯度的模长 $\\|\\nabla f(\\mathbf{x})\\|$ 表示函数在该方向上的变化率。\n",
    "\n",
    "#### **示例**（二元函数 $f(x, y) = x^2 + y^2 \\$）：\n",
    "\n",
    "$$\n",
    "\\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)^\\top = (2x, 2y)^\\top\n",
    "$$\n",
    "在点 $(1, 1)$，梯度为 $(2, 2)^\\top$，指向右上方（函数增长最快的方向）。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 梯度下降（Gradient Descent）的定义**\n",
    "梯度下降是一种 **迭代优化算法**，用于寻找多元函数 $f(\\mathbf{x})$ 的极小值点。其核心思想是沿着负梯度方向逐步调整参数，使函数值减小。\n",
    "\n",
    "#### **算法步骤**：\n",
    "1. **初始化**：选择初始点 $\\mathbf{x}^{(0)}$，设置学习率 $\\eta > 0$。\n",
    "2. **迭代更新**：对第 $k$ 次迭代：\n",
    "   $$\n",
    "   \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\eta \\cdot \\nabla f(\\mathbf{x}^{(k)})\n",
    "   $$\n",
    "3. **终止条件**：当梯度接近零（$\\|\\nabla f(\\mathbf{x}^{(k)})\\| < \\epsilon$）或达到最大迭代次数时停止。\n",
    "\n",
    "#### **几何解释**：\n",
    "- 每一步向函数值下降最快的方向（负梯度方向）移动。\n",
    "- 学习率 $\\eta$ 控制步长：过大可能发散，过小收敛慢。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 梯度下降的数学推导**\n",
    "假设目标是找到 $f(\\mathbf{x})$ 的极小值，根据泰勒展开的一阶近似：\n",
    "$$\n",
    "f(\\mathbf{x} + \\Delta \\mathbf{x}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top \\Delta \\mathbf{x}\n",
    "$$\n",
    "为使 $f(\\mathbf{x} + \\Delta \\mathbf{x}) < f(\\mathbf{x})$，选择：\n",
    "$$\n",
    "\\Delta \\mathbf{x} = -\\eta \\cdot \\nabla f(\\mathbf{x})\n",
    "$$\n",
    "此时：\n",
    "$$\n",
    "f(\\mathbf{x} + \\Delta \\mathbf{x}) \\approx f(\\mathbf{x}) - \\eta \\|\\nabla f(\\mathbf{x})\\|^2\n",
    "$$\n",
    "由于 $\\|\\nabla f(\\mathbf{x})\\|^2 \\geq 0$，函数值必然下降（当梯度非零时）。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 梯度下降的变体**\n",
    "| **方法**          | **更新规则**                                                                 | **特点**                              |\n",
    "|--------------------|-----------------------------------------------------------------------------|---------------------------------------|\n",
    "| **批量梯度下降**  | 使用全部训练数据计算梯度：$\\nabla f(\\mathbf{x}) = \\frac{1}{N}\\sum_{i=1}^N \\nabla f_i(\\mathbf{x})$ | 稳定但计算成本高                      |\n",
    "| **随机梯度下降**  | 每次随机选一个样本计算梯度：$\\nabla f_i(\\mathbf{x})$                     | 收敛快但波动大                        |\n",
    "| **小批量梯度下降**| 每次选一个小批量样本计算梯度：$\\frac{1}{m}\\sum_{i=1}^m \\nabla f_i(\\mathbf{x})$ | 平衡计算效率和收敛稳定性              |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 梯度下降的示例**\n",
    "以最小化 $f(x, y) = x^2 + 3y^2$ 为例：\n",
    "1. **梯度计算**：\n",
    "   $$\n",
    "   \\nabla f(x, y) = (2x, 6y)^\\top\n",
    "   $$\n",
    "2. **迭代过程**（设初始点 $(2, 1)$，学习率 $\\eta = 0.1 $）：\n",
    "   - **第1步**：\n",
    "     $$\n",
    "     \\mathbf{x}^{(1)} = (2, 1) - 0.1 \\cdot (4, 6) = (2 - 0.4, 1 - 0.6) = (1.6, 0.4)\n",
    "     $$\n",
    "   - **第2步**：\n",
    "     $$\n",
    "     \\nabla f(1.6, 0.4) = (3.2, 2.4)^\\top\n",
    "     $$\n",
    "     $$\n",
    "     \\mathbf{x}^{(2)} = (1.6, 0.4) - 0.1 \\cdot (3.2, 2.4) = (1.28, 0.16)\n",
    "     $$\n",
    "   持续迭代，最终逼近极小值点 $(0, 0)$。\n",
    "\n",
    "---\n",
    "\n",
    "### **6. 梯度下降的局限性**\n",
    "1. **局部极小值**：可能收敛到局部极小值而非全局极小值。\n",
    "2. **鞍点问题**：在高维空间中，梯度为零的点可能是鞍点（某些方向上升，某些方向下降）。\n",
    "3. **学习率选择**：需手动调整学习率，自适应方法（如Adam）可缓解此问题。\n",
    "\n",
    "---\n",
    "\n",
    "### **7. 在机器学习中的应用**\n",
    "- **线性回归**：最小化均方误差 $J(\\mathbf{w}) = \\frac{1}{2N}\\sum_{i=1}^N (y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2$。\n",
    "- **神经网络**：通过反向传播计算梯度，更新权重。\n",
    "- **支持向量机**：优化间隔损失函数。\n",
    "\n",
    "---\n",
    "\n",
    "### **总结**\n",
    "- **梯度**是多元函数变化率最大的方向向量。\n",
    "- **梯度下降**通过负梯度方向迭代更新参数，是机器学习的核心优化工具。\n",
    "- 实际应用中需结合具体问题选择梯度下降的变体和学习率策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649bf653-3af3-4df8-80a1-403e34574d35",
   "metadata": {},
   "source": [
    "## **2. 梯度下降算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba644920-7499-43ac-9dd2-13398bd3e0ce",
   "metadata": {},
   "source": [
    "## **3. 散度（拉普拉斯算子）**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
