{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4470cfc-8f69-4531-89ac-16de97bbeb32",
   "metadata": {},
   "source": [
    "# **深度学习系列：梯度 & 散度**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c81b0a-cefa-4822-88a9-f5e84e538d1c",
   "metadata": {},
   "source": [
    "## **0. 目录**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fec4af-af57-4319-b4a9-01ce1275e252",
   "metadata": {},
   "source": [
    "## **1. 梯度**\n",
    "\n",
    "以下是关于 **多元函数的梯度** 和 **梯度下降** 的详细定义和解释，结合数学公式与几何意义说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83194b-b1e0-4ce8-b460-727181fc3ece",
   "metadata": {},
   "source": [
    "### **1.1 梯度（Gradient）的定义**\n",
    "\n",
    "对于多元函数 $f(\\mathbf{x})$，其中 $\\mathbf{x} = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$，其梯度是一个向量，由函数对所有自变量的偏导数组成：\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)^\\top \\quad \\text{or} \\quad \\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121e4ed-187a-45ad-bf73-5732830f5b06",
   "metadata": {},
   "source": [
    "### **1.2 几何意义**\n",
    "\n",
    "- 梯度方向是函数在该点 **增长最快** 的方向。\n",
    "- 梯度的模长 $\\|\\nabla f(\\mathbf{x})\\|$ 表示函数在该方向上的变化率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c484c-14d5-4f6d-b6c2-323839773dde",
   "metadata": {},
   "source": [
    "### **1.3 示例**（二元函数 $f(x, y) = x^2 + y^2 \\$）\n",
    "\n",
    "$$\n",
    "\\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)^\\top = (2x, 2y)^\\top\n",
    "$$\n",
    "在点 $(1, 1)$，梯度为 $(2, 2)^\\top$，指向右上方（函数增长最快的方向）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ab056-153d-4752-9a6c-9134196e3ae9",
   "metadata": {},
   "source": [
    "## **2. 梯度下降（Gradient Descent）的定义与算法**\n",
    "\n",
    "梯度下降是一种 **迭代优化算法**，用于寻找多元函数 $f(\\mathbf{x})$ 的极小值点。其核心思想是沿着负梯度方向逐步调整参数，使函数值减小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8ae37-8048-4b02-bfab-d595cc43a727",
   "metadata": {},
   "source": [
    "### **2.1 算法步骤**\n",
    "\n",
    "- **初始化**：选择初始点 $\\mathbf{x}^{(0)}$，设置学习率 $\\eta > 0$。\n",
    "- **迭代更新**：对第 $k$ 次迭代：\n",
    "  $$\n",
    "  \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\eta \\cdot \\nabla f(\\mathbf{x}^{(k)})\n",
    "  $$\n",
    "- **终止条件**：当梯度接近零（$\\|\\nabla f(\\mathbf{x}^{(k)})\\| < \\epsilon$）或达到最大迭代次数时停止。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a6ac3d-0607-47f1-af45-1f51a8f74e72",
   "metadata": {},
   "source": [
    "### **2.2 几何解释**\n",
    "\n",
    "- 每一步向函数值下降最快的方向（负梯度方向）移动。\n",
    "- 学习率 $\\eta$ 控制步长：过大可能发散，过小收敛慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3a91f-66a7-4124-aaad-d6464426a7a3",
   "metadata": {},
   "source": [
    "### **2.3 梯度下降的数学推导**\n",
    "\n",
    "假设目标是找到 $f(\\mathbf{x})$ 的极小值，根据泰勒展开的一阶近似：\n",
    "$$\n",
    "f(\\mathbf{x} + \\Delta \\mathbf{x}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top \\Delta \\mathbf{x}\n",
    "$$\n",
    "为使 $f(\\mathbf{x} + \\Delta \\mathbf{x}) < f(\\mathbf{x})$，选择：\n",
    "$$\n",
    "\\Delta \\mathbf{x} = -\\eta \\cdot \\nabla f(\\mathbf{x})\n",
    "$$\n",
    "此时：\n",
    "$$\n",
    "f(\\mathbf{x} + \\Delta \\mathbf{x}) \\approx f(\\mathbf{x}) - \\eta \\|\\nabla f(\\mathbf{x})\\|^2\n",
    "$$\n",
    "由于 $\\|\\nabla f(\\mathbf{x})\\|^2 \\geq 0$，函数值必然下降（当梯度非零时）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd5b3d-0d65-4b32-b5a1-997557143ef6",
   "metadata": {},
   "source": [
    "### **2.4 梯度下降的变体**\n",
    "\n",
    "| **方法**          | **更新规则**                                                                 | **特点**                              |\n",
    "|--------------------|-----------------------------------------------------------------------------|---------------------------------------|\n",
    "| **批量梯度下降**  | 使用全部训练数据计算梯度：$\\nabla f(\\mathbf{x}) = \\frac{1}{N}\\sum_{i=1}^N \\nabla f_i(\\mathbf{x})$ | 稳定但计算成本高                      |\n",
    "| **随机梯度下降**  | 每次随机选一个样本计算梯度：$\\nabla f_i(\\mathbf{x})$                     | 收敛快但波动大                        |\n",
    "| **小批量梯度下降**| 每次选一个小批量样本计算梯度：$\\frac{1}{m}\\sum_{i=1}^m \\nabla f_i(\\mathbf{x})$ | 平衡计算效率和收敛稳定性              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800f70d-8635-4ed2-8995-d553451f631e",
   "metadata": {},
   "source": [
    "### **2.5 梯度下降的示例**\n",
    "\n",
    "以最小化 $f(x, y) = x^2 + 3y^2$ 为例：\n",
    "\n",
    "- **梯度计算**：\n",
    "  $$\n",
    "  \\nabla f(x, y) = (2x, 6y)^\\top\n",
    "  $$\n",
    "- **迭代过程**（设初始点 $(2, 1)$，学习率 $\\eta = 0.1 $）：\n",
    "\n",
    "   - **第1步**：\n",
    "     $$\n",
    "     \\mathbf{x}^{(1)} = (2, 1) - 0.1 \\cdot (4, 6) = (2 - 0.4, 1 - 0.6) = (1.6, 0.4)\n",
    "     $$\n",
    "   - **第2步**：\n",
    "     $$\n",
    "     \\nabla f(1.6, 0.4) = (3.2, 2.4)^\\top\n",
    "     $$\n",
    "     $$\n",
    "     \\mathbf{x}^{(2)} = (1.6, 0.4) - 0.1 \\cdot (3.2, 2.4) = (1.28, 0.16)\n",
    "     $$\n",
    "   持续迭代，最终逼近极小值点 $(0, 0)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41344fb2-cca4-4986-b0c7-208fa758b163",
   "metadata": {},
   "source": [
    "### **2.6 梯度下降的局限性**\n",
    "\n",
    "- **局部极小值**：可能收敛到局部极小值而非全局极小值。\n",
    "- **鞍点问题**：在高维空间中，梯度为零的点可能是鞍点（某些方向上升，某些方向下降）。\n",
    "- **学习率选择**：需手动调整学习率，自适应方法（如Adam）可缓解此问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8b39d-28fa-48e6-82ad-f168e80cdf38",
   "metadata": {},
   "source": [
    "### **2.7 在机器学习中的应用**\n",
    "\n",
    "- **线性回归**：最小化均方误差 $J(\\mathbf{w}) = \\frac{1}{2N}\\sum_{i=1}^N (y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2$。\n",
    "- **神经网络**：通过反向传播计算梯度，更新权重。\n",
    "- **支持向量机**：优化间隔损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd8bf0-6bb0-4334-9b4b-5e20e607ca23",
   "metadata": {},
   "source": [
    "### **总结**\n",
    "\n",
    "- **梯度**是多元函数变化率最大的方向向量。\n",
    "- **梯度下降**通过负梯度方向迭代更新参数，是机器学习的核心优化工具。\n",
    "- 实际应用中需结合具体问题选择梯度下降的变体和学习率策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba644920-7499-43ac-9dd2-13398bd3e0ce",
   "metadata": {},
   "source": [
    "## **3. 散度（拉普拉斯算子）**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b6887-ca36-4188-889c-7b859cb7eebc",
   "metadata": {},
   "source": [
    "### **3.1 定义**\n",
    "\n",
    "拉普拉斯算子（Laplacian /lɑ:'plɑ:siən/）是 $n$ 维欧几里得空间中的一个**二阶微分算子**，定义为函数的梯度（$\\nabla f$）的散度（$\\nabla^2 f$）。\n",
    "\n",
    "以二元函数 $f(x, y)$ 为例，其拉普拉斯算子（Laplacian）是一个**二阶微分算子**，定义为函数的梯度的散度。在二维空间中，拉普拉斯算子的表达式为：\n",
    "\n",
    "$$\n",
    "\\nabla^2 f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $\\frac{\\partial^2 f}{\\partial x^2}$ 是函数 $f$ 对变量 $x$ 的二阶偏导数。\n",
    "- $\\frac{\\partial^2 f}{\\partial y^2}$ 是函数 $f$ 对变量 $y$ 的二阶偏导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225d0e1-56cd-4368-a361-abd300b15d4d",
   "metadata": {},
   "source": [
    "#### 💥**注意：导数与微分的联系及区别**\n",
    "\n",
    "**导数：**\n",
    "\n",
    "导数是一个函数在某一点处的瞬时变化率，它描述了函数在该点附近的局部行为。对于一元函数 $f(x)$， 其在点 $x$ 处的导数定义为：\n",
    "    $$\n",
    "    f^{'}(x) = \\lim_{\\Delta x \\to 0 } \\frac{f(x + \\Delta x)-f(x)}{\\Delta x}\n",
    "    $$\n",
    "\n",
    "如果这个极限存在，它给出了函数 $f$ 在点 $x$ 处的切线斜率。导数可以用于描述速度、加速度、优化问题中的极值点等。\n",
    "\n",
    "**微分：**\n",
    "\n",
    "微分是函数在某一点处的线性近似，它描述了函数在该点附近对的局部线性变化。对于一元函数 $f(x)$，其在点 $x$ 处的微分 $df$ 定义为：$df = f^{'}(x) \\Delta x$\n",
    "\n",
    "这里 $\\Delta x$ 是 $x$ 的一个小变化量。微分 $df$ 表示当 $x$ 变化 $\\Delta x$ 时，函数 $f$ 的近似变化量。\n",
    "\n",
    "**导数与微分的关系：**\n",
    "\n",
    "- 导数是微分的系数：在微分 $df = f^{'}(x) \\Delta x$ 中，导数 $f^{'}(x)$ 是微分 $df$ 相对于 $\\Delta x$ 的系数。这里意味着导数给出了函数在该点处的最佳线性近似的斜率。\n",
    "- 微分是导数的应用：微分是利用导数来近似计算函数值变化的一种方法。当我们只知道函数在某点的导数，但不知道函数的具体形式时，可以通过微分来估计函数值的变化。\n",
    "- 几何解释：在几何上，导数对应于函数图像在该点处的切线斜率，而微分则对应于切线上的一小段长度，这段长度是 $\\Delta x$ 在切线上的投影。\n",
    "- 计算关系：在实际计算中，我们通常先计算函数的导数，然后利用导数来计算微分。例如，若 $f(x) = x^2$，那么 $f^{'} = 2x$，因此微分 $df = 2x \\Delta x$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec497d4f-16fc-4881-8c84-5d5915f3abea",
   "metadata": {},
   "source": [
    "### **3.2 示例**\n",
    "\n",
    "假设有一个二元函数 $f(x, y) = x^2 + y^2$，计算它的拉普拉斯算子：\n",
    "\n",
    "- **计算一阶偏导数**：\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 2y\n",
    "  $$\n",
    "\n",
    "- **计算二阶偏导数**：\n",
    "  $$\n",
    "  \\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad \\frac{\\partial^2 f}{\\partial y^2} = 2\n",
    "  $$\n",
    "\n",
    "- **计算拉普拉斯算子**：\n",
    "  $$\n",
    "  \\nabla^2 f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2} = 2 + 2 = 4\n",
    "  $$\n",
    "\n",
    "因此，函数 $f(x, y) = x^2 + y^2$ 的拉普拉斯算子为 4。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e895646-3c71-4049-8884-f36758e52c7f",
   "metadata": {},
   "source": [
    "### **3.3 数学及物理意义**\n",
    "\n",
    "在拉普拉斯算子中，二阶偏导数起着至关重要的作用。它们反映了函数在每个坐标方向上的**曲率**变化，从而帮助我们理解函数在空间中的整体行为。以下是二阶偏导数在拉普拉斯算子中的具体作用："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff5647-8014-44f1-bbc4-3cd6e21df69c",
   "metadata": {},
   "source": [
    "#### **3.3.1 曲率衡量**\n",
    "\n",
    "二阶偏导数衡量了函数在某个方向上的曲率。例如，对于二元函数 $f(x, y)$，二阶偏导数 $\\frac{\\partial^2 f}{\\partial x^2}$ 表示函数在 $x$ 方向上的曲率，而 $\\frac{\\partial^2 f}{\\partial y^2}$ 表示函数在 $y$ 方向上的曲率。\n",
    "\n",
    "- **正的二阶偏导数**：表示函数在该方向上是向上凸的（即曲率向上）。\n",
    "- **负的二阶偏导数**：表示函数在该方向上是向下凹的（即曲率向下）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a08f21-4efd-44dc-87ed-84d7c4e5eb6d",
   "metadata": {},
   "source": [
    "#### **3.3.2 整体变化趋势**\n",
    "\n",
    "拉普拉斯算子将所有二阶偏导数相加，从而反映了函数在所有坐标方向上的整体变化趋势。具体来说：\n",
    "\n",
    "- **正的拉普拉斯值**：表示函数在该点的整体趋势是向上的，即函数在该点周围是局部凸的。\n",
    "- **负的拉普拉斯值**：表示函数在该点的整体趋势是向下的，即函数在该点周围是局部凹的。\n",
    "- **零的拉普拉斯值**：表示函数在该点的曲率在各个方向上相互抵消，整体上没有明显的凸凹性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e11d764-5640-4435-a758-c7a5cd279cdf",
   "metadata": {},
   "source": [
    "#### **3.3.3 数学性质**\n",
    "\n",
    "从数学的角度来看，二阶偏导数在拉普拉斯算子中的作用还包括：\n",
    "\n",
    "- **对称性**：拉普拉斯算子是一个标量算子，它不依赖于坐标系的选择。这意味着无论坐标系如何旋转，拉普拉斯算子的值保持不变。\n",
    "- **线性性**：拉普拉斯算子是一个线性算子，即对于任意两个函数 $f$ 和 $g$，以及常数 $a$ 和 $b$，有：\n",
    "  $$\n",
    "  \\nabla^2 (af + bg) = a \\nabla^2 f + b \\nabla^2 g\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b0138-9718-4b69-b821-7d6c9c3bdc48",
   "metadata": {},
   "source": [
    "#### **3.3.4 物理意义**\n",
    "\n",
    "在物理学中，拉普拉斯算子常用于描述各种场的性质，二阶偏导数在其中的作用可以理解为：\n",
    "\n",
    "- **热传导方程**：拉普拉斯算子描述了温度场在空间中的扩散情况。正的拉普拉斯值表示该点温度高于周围区域，热量会从该点向周围扩散；负的拉普拉斯值则相反。\n",
    "- **电磁学**：在静电场中，拉普拉斯算子用于描述电势的分布。拉普拉斯方程 $\\nabla^2 \\phi = 0$ 表示在没有电荷的区域，电势是调和函数，即电势在每个点的曲率在各个方向上相互抵消。\n",
    "- **流体力学**：在势流理论中，拉普拉斯算子用于描述速度势函数。拉普拉斯方程 $\\nabla^2 \\phi = 0$ 表示流体的流动是无旋的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd835b-c918-4aa4-b22a-a62455f7e22f",
   "metadata": {},
   "source": [
    "#### **3.3.5 示例**\n",
    "\n",
    "以二元函数 $f(x, y) = x^2 + y^2$ 为例：\n",
    "\n",
    "- 二阶偏导数 $\\frac{\\partial^2 f}{\\partial x^2} = 2$ 和 $\\frac{\\partial^2 f}{\\partial y^2} = 2$。\n",
    "- 拉普拉斯算子 $\\nabla^2 f = 2 + 2 = 4 $。\n",
    "\n",
    "这表明函数 $f(x, y) = x^2 + y^2$ 在每个点的曲率都是向上的，整体趋势是凸的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051066af-ac6a-4fa3-aad7-46c8ffaed771",
   "metadata": {},
   "source": [
    "通过理解二阶偏导数在拉普拉斯算子中的作用，我们可以更好地分析函数的几何形状和物理意义。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
